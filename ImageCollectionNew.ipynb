{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import cv2\n",
    "import uuid\n",
    "import time\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_imgs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobilenet-hand' \n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
    "IMAGES_PATH = os.path.join('Tensorflow', 'workspace', 'images', 'collectedimages')\n",
    "LABELIMG_PATH = os.path.join('Tensorflow', 'labelimg')\n",
    "ACTIONS_RECORD = os.path.join('Tensorflow', 'workspace', 'actions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow_hand', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow_hand','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow_hand','models'),\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow_hand', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow_hand', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow_hand', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow_hand', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow_hand', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow_hand', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'TFJS_PATH':os.path.join('Tensorflow_hand', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n",
    "    'TFLITE_PATH':os.path.join('Tensorflow_hand', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
    "    'PROTOC_PATH':os.path.join('Tensorflow_hand','protoc')\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow_hand', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'how', 'you', 'thanks']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "with open(ACTIONS_RECORD, 'r') as file:\n",
    "    labels = file.read().splitlines()\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        if os.name == 'posix':\n",
    "            !mkdir -p {path}\n",
    "        if os.name == 'nt':\n",
    "            !mkdir {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "    from object_detection.utils import label_map_util\n",
    "    from object_detection.utils import visualization_utils as viz_utils\n",
    "    from object_detection.builders import model_builder\n",
    "    from object_detection.utils import config_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "    configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "    detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "    # Restore checkpoint\n",
    "    ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "    ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-6')).expect_partial()\n",
    "\n",
    "    @tf.function\n",
    "    def detect_fn(image):\n",
    "        image, shapes = detection_model.preprocess(image)\n",
    "        prediction_dict = detection_model.predict(image, shapes)\n",
    "        detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create an annotation XML file\n",
    "\n",
    "def create_annotation_file(action, filename, path, xmin, ymin, xmax, ymax):\n",
    "    annotation = ET.Element(\"annotation\")\n",
    "\n",
    "    folder = ET.SubElement(annotation, \"folder\")\n",
    "    folder.text = \"collectedimages\"\n",
    "\n",
    "    filename_element = ET.SubElement(annotation, \"filename\")\n",
    "    filename_element.text = filename\n",
    "\n",
    "    path_element = ET.SubElement(annotation, \"path\")\n",
    "    path_element.text = \"C:\\\\Users\\\\anujv\\\\Desktop\\\\EPICS\\\\\" + path\n",
    "\n",
    "    source = ET.SubElement(annotation, \"source\")\n",
    "    database = ET.SubElement(source, \"database\")\n",
    "    database.text = \"Unknown\"\n",
    "\n",
    "    size = ET.SubElement(annotation, \"size\")\n",
    "    width = ET.SubElement(size, \"width\")\n",
    "    width.text = \"640\"  # Assuming width is fixed\n",
    "    height = ET.SubElement(size, \"height\")\n",
    "    height.text = \"480\"  # Assuming height is fixed\n",
    "    depth = ET.SubElement(size, \"depth\")\n",
    "    depth.text = \"3\"\n",
    "\n",
    "    segmented = ET.SubElement(annotation, \"segmented\")\n",
    "    segmented.text = \"0\"\n",
    "\n",
    "    # Object\n",
    "    object_element = ET.SubElement(annotation, \"object\")\n",
    "    name = ET.SubElement(object_element, \"name\")\n",
    "    name.text = action\n",
    "    pose = ET.SubElement(object_element, \"pose\")\n",
    "    pose.text = \"Unspecified\"\n",
    "    truncated = ET.SubElement(object_element, \"truncated\")\n",
    "    truncated.text = \"0\"\n",
    "    difficult = ET.SubElement(object_element, \"difficult\")\n",
    "    difficult.text = \"0\"\n",
    "    bndbox = ET.SubElement(object_element, \"bndbox\")\n",
    "    xmin_element = ET.SubElement(bndbox, \"xmin\")\n",
    "    xmin_element.text = str(xmin)\n",
    "    ymin_element = ET.SubElement(bndbox, \"ymin\")\n",
    "    ymin_element.text = str(ymin)\n",
    "    xmax_element = ET.SubElement(bndbox, \"xmax\")\n",
    "    xmax_element.text = str(xmax)\n",
    "    ymax_element = ET.SubElement(bndbox, \"ymax\")\n",
    "    ymax_element.text = str(ymax)\n",
    "\n",
    "    # Write the annotation to a file\n",
    "    annotation_file = os.path.join(IMAGES_PATH, filename.replace(\".jpg\", \".xml\"))\n",
    "    tree = ET.ElementTree(annotation)\n",
    "    tree.write(annotation_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def ask_to_save_image():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    answer = messagebox.askyesno(\"Save Image\", \"Do you want to save this image?\")\n",
    "\n",
    "    root.destroy()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m countdown_frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     19\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(np\u001b[38;5;241m.\u001b[39mexpand_dims(countdown_frame, \u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 20\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_fn\u001b[49m(input_tensor)\n\u001b[0;32m     22\u001b[0m num_detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(detections\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_detections\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     23\u001b[0m detections \u001b[38;5;241m=\u001b[39m {key: value[\u001b[38;5;241m0\u001b[39m, :num_detections]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     24\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m detections\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detect_fn' is not defined"
     ]
    }
   ],
   "source": [
    "# Input action from the user\n",
    "action = input('Enter action: ')\n",
    "no_of_images = 0\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Collect frames\n",
    "for imgnum in range(5):\n",
    "    ret, frame = cap.read()\n",
    "    cv2.putText(frame, f'Collecting frames for {action} - Image Number {imgnum}', (15, 12),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    for countdown in range(5, 0, -1):\n",
    "        countdown_frame = frame.copy()\n",
    "\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(countdown_frame, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "            \n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy()\n",
    "                    for key, value in detections.items()}\n",
    "        detections['num_detections'] = num_detections\n",
    "\n",
    "        # detection_classes should be ints.\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        label_id_offset = 1\n",
    "        image_np_with_detections = frame.copy()\n",
    "\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                    image_np_with_detections,\n",
    "                    detections['detection_boxes'],\n",
    "                    detections['detection_classes']+label_id_offset,\n",
    "                    detections['detection_scores'],\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates=True,\n",
    "                    max_boxes_to_draw=1,\n",
    "                    min_score_thresh=.8,\n",
    "                    agnostic_mode=False)\n",
    "            \n",
    "        image_np_with_detections_copy = image_np_with_detections.copy()\n",
    "        cv2.putText(image_np_with_detections_copy, f'Capturing in {countdown} seconds', (120, 200),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "            \n",
    "        cv2.imshow('Countdown', image_np_with_detections_copy)\n",
    "        cv2.waitKey(1000)\n",
    "\n",
    "    if imgnum != 0:\n",
    "        # After countdown, save the image and create the annotation file\n",
    "        most_accurate_box_index = np.argmax(detections['detection_scores'])\n",
    "        box = detections['detection_boxes'][most_accurate_box_index]\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "\n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        xmin_pixel = int(xmin * width)\n",
    "        ymin_pixel = int(ymin * height)\n",
    "        xmax_pixel = int(xmax * width)\n",
    "        ymax_pixel = int(ymax * height)\n",
    "\n",
    "        if ask_to_save_image():\n",
    "            filename = action + '.' + f'{str(uuid.uuid1())}.jpg'\n",
    "            path = os.path.join(IMAGES_PATH, filename)\n",
    "            cv2.imwrite(path, frame)\n",
    "            create_annotation_file(action, filename, path, xmin_pixel, ymin_pixel, xmax_pixel, ymax_pixel)\n",
    "            no_of_images += 1\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(no_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
